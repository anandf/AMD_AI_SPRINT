{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b4035aa-1b7f-4980-8bf5-898bebf0b0eb",
   "metadata": {},
   "source": [
    "# Workshop Optimized Model Serving with vLLM V1 with AMD GPUs\n",
    "\n",
    "Welcome to this hands-on workshop! \n",
    "\n",
    "You will run vLLM with v0 and new v1 features and compare performance characteristics. \n",
    "\n",
    "The overall lab architecture is\n",
    "![architecture](./assets/ws201_1.jpg)\n",
    "- ‚≠ê vLLM Server with <span style=\"color:red\">legacy v0</span> \n",
    "- ‚≠ê vLLM Server with <span style=\"color:red\">v1</span> **without** <span style=\"color:green\">Prefix caching</span> \n",
    "- ‚≠ê vLLM Server with <span style=\"color:red\">v1</span> **with** <span style=\"color:green\">Prefix caching</span>\n",
    "- üíª Jupyter Notebook Client: You'll remotely launch vLLM Servers and measure performance here\n",
    "\n",
    "In this workshop, you will launch vLLM Servers on AMD MI300X GPUs and record and compare LLM performance metrics.\n",
    "![llm_metrics](./assets/ws201_2.jpg)\n",
    "\n",
    "- üö©**TTFT** and **TPOT** are key LLM latency metrics, <span style=\"color:green\">_Shorter is better_ </span> \n",
    "- üö©**TOTAL_TPS** (Total Token Per Second) is a LLM throughput metric , <span style=\"color:green\">_Higher is better_ </span> \n",
    "\n",
    "To do so, you need to <span style=\"color:red\">iterate **three times** </span>of launching the vLLM server and measuring performance. \n",
    "\n",
    "Let's dive in and see the benefits of <span style=\"color:red\"> vLLM v1</span> !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe5ffe-ebbd-448f-8926-dcedce4f10ba",
   "metadata": {},
   "source": [
    "## Please run these helper functions first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ceb24-8085-442d-a68e-8add8c1ba9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_status(port):\n",
    "    import subprocess\n",
    "    log_labels = f'''\n",
    "        #!/usr/bin/bash\n",
    "        curl -s http://localhost:{port}/v1/models > /dev/null\n",
    "        '''\n",
    "    subprocess.run(log_labels, shell=True, check=True)\n",
    "\n",
    "def server_llm_resp(port):\n",
    "    try:\n",
    "        server_status(port)\n",
    "    except:\n",
    "        print(\"Please launch vLLM server at port \"+str(port)+ \" first\")\n",
    "        return 0\n",
    "        \n",
    "    from openai import OpenAI\n",
    "    # Set OpenAI's API key and API base to use vLLM's API server.\n",
    "    openai_api_key = \"EMPTY\"\n",
    "    openai_api_base = \"http://localhost:\"+str(port)+\"/v1\"\n",
    "    client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        base_url=openai_api_base,\n",
    "    )\n",
    "    chat_response = client.chat.completions.create(\n",
    "    model=\"RedHatAI/Llama-3.1-8B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "        ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    print(\"RESPONSE:\\n %10s\"%(chat_response.choices[0].message.content))\n",
    "\n",
    "def run_bench(SRC_LOG, TGT_LOG, isl, osl, con_list, case, port):\n",
    "    try:\n",
    "        server_status(port)\n",
    "    except:\n",
    "        print(\"Please launch vLLM server at port \"+str(port)+ \" first\")\n",
    "        return 0\n",
    "    import subprocess\n",
    "    log_labels = f'''\n",
    "        #!/usr/bin/bash\n",
    "        printf run,                     2>&1 | tee -a {TGT_LOG}\n",
    "        printf prompts,                 2>&1 | tee -a {TGT_LOG}\n",
    "        printf median_ttft,             2>&1 | tee -a {TGT_LOG}\n",
    "        printf median_tpot,             2>&1 | tee -a {TGT_LOG}\n",
    "        printf median_e2e,              2>&1 | tee -a {TGT_LOG}\n",
    "        printf total_tps                2>&1 | tee -a {TGT_LOG}\n",
    "        printf \"\\n\"                     2>&1 | tee -a {TGT_LOG}\n",
    "        '''\n",
    "    subprocess.run(log_labels, shell=True, check=True)\n",
    "    for concurrency in con_list:\n",
    "        prompts = 4 * concurrency\n",
    "\n",
    "        vllm_run = f'''\n",
    "            #!/usr/bin/env bash\n",
    "            VLLM_LOGGING_LEVEL=ERROR \\\n",
    "            python3 /app/vllm/benchmarks/benchmark_serving.py \\\n",
    "                --model RedHatAI/Llama-3.1-8B-Instruct \\\n",
    "                --dataset-name random \\\n",
    "                --random-input-len {isl} \\\n",
    "                --random-output-len {osl} \\\n",
    "                --num-prompts {prompts} \\\n",
    "                --max-concurrency {concurrency} \\\n",
    "                --ignore-eos \\\n",
    "                --port {port} \\\n",
    "                --percentile-metrics ttft,tpot,e2el \\\n",
    "                2>&1 | tee {SRC_LOG}\n",
    "            '''\n",
    "        log_post_process = f'''\n",
    "            #!/usr/bin/bash\n",
    "            bash ./rpt_sum.sh {SRC_LOG} {TGT_LOG} {case}\n",
    "            '''\n",
    "\n",
    "        subprocess.run(vllm_run, shell=True, check=True)\n",
    "        subprocess.run(log_post_process, shell=True, check=True)\n",
    "\n",
    "def visualize_bench(logs):\n",
    "    !pip install matplotlib -q\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    df_sum = pd.DataFrame()\n",
    "\n",
    "    for log in logs:\n",
    "        df_log = pd.read_csv(log, sep=',')\n",
    "        print(df_log)\n",
    "        df_sum = pd.concat([df_sum, df_log])\n",
    "\n",
    "    fig, axes = plt.subplots(figsize=(16,8),nrows=1, ncols=3)\n",
    "\n",
    "    df_sum_pivot=df_sum.pivot(index='prompts', columns='run', values='median_ttft')\n",
    "    df_sum_pivot.plot.bar(rot=0, title='median_ttft (ms), lower is better', ax=axes[0])\n",
    "\n",
    "    df_sum_pivot=df_sum.pivot(index='prompts', columns='run', values='median_tpot')\n",
    "    df_sum_pivot.plot.bar(rot=0, title='median_tpot (ms), lower is better', ax=axes[1])\n",
    "\n",
    "    ax = plt.gca() \n",
    "    ax.set_facecolor(\"pink\")\n",
    "    df_sum_pivot=df_sum.pivot(index='prompts', columns='run', values='total_tps')\n",
    "    df_sum_pivot.plot.bar(rot=0, title='total_tps (tok/sec), higher is better', ax=axes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1830a1bc-49a7-4e97-92cc-399b919c059e",
   "metadata": {},
   "source": [
    "## Customize Jupyter notebook screen\n",
    "\n",
    "To show multiple terminals and ipynb notebook files, you can customize the screen by dragging and dropping. \n",
    "\n",
    "We recommend you to have 2 terminals and 1 jupyter notebook. \n",
    "\n",
    "- 1st terminal: ```watch -n 1 rocm-smi```\n",
    "- 2nd terminal: vLLM server command copying and pasting\n",
    "- 3rd jupyter notebook (notebook.ipynb) : This notebook\n",
    "  \n",
    "![llm_metrics](./assets/ws201_3.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2384146-3b57-4095-9b10-cd272a3e5b0d",
   "metadata": {},
   "source": [
    "# STEP 1 vLLM v0 default Performance Benchmark\n",
    "\n",
    "<span style=\"color:blue\"><strong>‚ö†Ô∏è WARNING:</strong></span> Copy and paste this server command in the terminal\n",
    "\n",
    "```sh\n",
    "VLLM_USE_V1=0 \\\n",
    "VLLM_LOGGING_LEVEL=INFO \\\n",
    "vllm serve RedHatAI/Llama-3.1-8B-Instruct \\\n",
    "            --disable-log-requests \\\n",
    "            --trust-remote-code -tp 1 \\\n",
    "            --cuda-graph-sizes 64 \\\n",
    "            --port 8001 \\\n",
    "            --chat-template /app/vllm/examples/tool_chat_template_llama3.1_json.jinja\n",
    "```\n",
    "\n",
    "- üìåNotice: <span style=\"color:green\">```VLLM_USE_V1=0```</span> is an environment variable to let vLLM run in v0 mode.\n",
    "- üìåNotice: Use <span style=\"color:green\">```--port 8001```</span>in v0 mode. \n",
    "\n",
    "From vLLM server, <span style=\"color:blue\"><strong>‚ö†Ô∏è WARNING:</strong></span> you should see these messages first. \n",
    "\n",
    "<span style=\"color:red\"> *INFO:     Started server process [210]*</span>\n",
    "\n",
    "<span style=\"color:red\"> *INFO:     Waiting for application startup.*</span>\n",
    "\n",
    "<span style=\"color:red\"> *INFO:     Application startup complete.*</span>\n",
    "\n",
    "Then run the following cells and check TTFT, TPOT, and TOTAL_TPS metrics vLLM v0 of RedHatAI/Llama-3.1-8B-Instruct on a single MI300X GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9e2cb9-1677-41bb-a891-d6f001136eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-1)  When vLLM server is ready, check the answer of \"Tell me a joke\"\n",
    "port=8001 # vlLM v0 port\n",
    "\n",
    "server_llm_resp(port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb09ee-ff84-4540-94c7-531272fdb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2) Run Benchmark: input_len/output_len = 1024/1024, concurrency = 32 and 64\n",
    "port=8001 # vlLM v0 port\n",
    "\n",
    "SRC_LOG=\"v0.log\"\n",
    "TGT_LOG=\"v0_summary.log\"\n",
    "case=\"v0\"\n",
    "!rm -f v0_summary.log\n",
    "run_bench(SRC_LOG, TGT_LOG, 1024, 1024, [32], case, port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5d8848-5dc1-423f-ab90-f1ab598c232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-3) Visualize Benchmarks\n",
    "try:\n",
    "    logs = [\n",
    "        \"v0_summary.log\",\n",
    "        ]\n",
    "    visualize_bench(logs)\n",
    "except:\n",
    "    print(\"Please rerun the previous step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625d31fe-20a4-44e6-9ac1-ddfafc561f3e",
   "metadata": {},
   "source": [
    "# STEP 2 vLLM v1 default (Chunked-prefill + Prefix Caching) Performance Benchmark\n",
    "\n",
    "<span style=\"color:blue\"><strong>‚ö†Ô∏è WARNING:</strong></span> ```Ctrl + C``` in the terminal and close the previous vLLM engine and copy and paste this server command in the terminal\n",
    "\n",
    "```sh\n",
    "VLLM_USE_V1=1 \\\n",
    "VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1 \\\n",
    "VLLM_LOGGING_LEVEL=INFO \\\n",
    "vllm serve RedHatAI/Llama-3.1-8B-Instruct \\\n",
    "            --disable-log-requests \\\n",
    "            --trust-remote-code -tp 1 \\\n",
    "            --cuda-graph-sizes 64 \\\n",
    "            --port 8003 \\\n",
    "            --chat-template /app/vllm/examples/tool_chat_template_llama3.1_json.jinja \n",
    "\n",
    "```\n",
    "\n",
    "- üìåNotice: <span style=\"color:green\">```VLLM_USE_V1=1```</span> is an environment variable to let vLLM run in v1 mode.\n",
    "- üìåNotice: <span style=\"color:green\">```--enable-prefix-caching```</span> is a vLLM argument to <span style=\"color:green\">enable</span> prefix-caching.\n",
    "- üìåNotice: Use <span style=\"color:green\">```--port 8003```</span>in v1 + prefix caching mode. \n",
    "\n",
    "From vLLM server, <span style=\"color:blue\"><strong>‚ö†Ô∏è WARNING:</strong></span> you should see these messages first.\n",
    "\n",
    "<span style=\"color:red\"> *INFO:     Started server process [210]*</span>\n",
    "\n",
    "<span style=\"color:red\"> *INFO:     Waiting for application startup.*</span>\n",
    "\n",
    "<span style=\"color:red\"> *INFO:     Application startup complete.*</span>\n",
    "\n",
    "Then run the following cells and check TTFT, TPOT, and TOTAL_TPS metrics vLLM v1 and also prefix caching of RedHatAI/Llama-3.1-8B-Instruct on a single MI300X GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65610231-0fdd-4b2e-89ad-6c3affc80c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-1) When vLLM server is ready, check the answer of \"Tell me a joke\"\n",
    "port=8003 # vlLM v1 + prefix caching port\n",
    "\n",
    "server_llm_resp(port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de179d3-cc81-425a-87b6-e651e3d6950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-2) Run Benchmark: input_len/output_len = 1024/1024, concurrency = 32 and 64\n",
    "port=8003 # vlLM v1 + prefix caching port\n",
    "\n",
    "SRC_LOG=\"v1.log\"\n",
    "TGT_LOG=\"v1_summary.log\"\n",
    "case=\"v1\"\n",
    "!rm -f v1_summary.log\n",
    "run_bench(SRC_LOG, TGT_LOG, 1024, 1024, [32], case, port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e7abf-2d2e-48df-a813-f66f15542fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3) Visualize Benchmarks\n",
    "try:\n",
    "    logs = [\n",
    "        \"v0_summary.log\",\n",
    "        \"v1_summary.log\",\n",
    "        ]\n",
    "    visualize_bench(logs)\n",
    "except:\n",
    "    print(\"Please rerun the previous step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f51051-d4af-40fc-9817-a079d658600b",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">**CHALLENGE**</span> Hand Craft v0 options to perform similar or better \"total_tps\" than  vLLM v1 \n",
    "\n",
    "<span style=\"color:blue\"><strong>‚ö†Ô∏è WARNING:</strong></span> ```Ctrl + C``` in the terminal and close the previous vLLM engine and copy and paste this server command in the terminal\n",
    "\n",
    "```sh\n",
    "VLLM_USE_V1=0 \\\n",
    "VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1 \\\n",
    "VLLM_LOGGING_LEVEL=INFO \\\n",
    "VLLM_USE_TRITON_FLASH_ATTN=0 \\\n",
    "vllm serve RedHatAI/Llama-3.1-8B-Instruct \\\n",
    "            --disable-log-requests \\\n",
    "            --trust-remote-code -tp 1 \\\n",
    "            --cuda-graph-sizes 64 \\\n",
    "            --port 8004 \\\n",
    "            --chat-template /app/vllm/examples/tool_chat_template_llama3.1_json.jinja \\\n",
    "            {add options!}\n",
    "```\n",
    "\n",
    "### <span style=\"color:red\">**HINT**</span> vllm Options can be found: https://github.com/vllm-project/vllm/blob/19108ef31191e217766ffe52e8e382ddbec20fdb/vllm/engine/arg_utils.py#L484\n",
    "\n",
    "- üìåNotice: Use <span style=\"color:green\">```--port 8004```</span>in challenge mode. \n",
    "\n",
    "From vLLM server, <span style=\"color:blue\"><strong>‚ö†Ô∏è WARNING:</strong></span> you should see these messages first.\n",
    "\n",
    "<span style=\"color:red\"> *INFO:     Started server process [210]*</span>\n",
    "\n",
    "<span style=\"color:red\"> *INFO:     Waiting for application startup.*</span>\n",
    "\n",
    "<span style=\"color:red\"> *INFO:     Application startup complete.*</span>\n",
    "\n",
    "Then run the following cells and check TTFT, TPOT, and TOTAL_TPS metrics vLLM v1 and also prefix caching of RedHatAI/Llama-3.1-8B-Instruct on a single MI300X GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72514c9f-ece9-4345-8c25-19b3be47cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-1) Run Benchmark: input_len/output_len = 1024/1024, concurrency = 32 and 64\n",
    "port=8004 # vlLM v1 + prefix caching port\n",
    "\n",
    "SRC_LOG=\"v0CHALLENGE.log\"\n",
    "TGT_LOG=\"v0CHALLENGE_summary.log\"\n",
    "case=\"v0CHALLENGE\"\n",
    "!rm -f v0CHALLENGE_summary.log\n",
    "run_bench(SRC_LOG, TGT_LOG, 1024, 1024, [32], case, port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808803bb-3412-42e8-8b4b-951c3e0cb408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-2) Visualize Benchmarks\n",
    "try:\n",
    "    logs = [\n",
    "        \"v0_summary.log\",\n",
    "        \"v1_summary.log\",\n",
    "        \"v0CHALLENGE_summary.log\"\n",
    "        ]\n",
    "    visualize_bench(logs)\n",
    "except:\n",
    "    print(\"Please rerun the previous step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f04fde-22f9-4832-8224-47d8b1e6b4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
